{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample package\n",
    "===============\n",
    "\n",
    "My own sample R package implementing sumx function with rPython library: https://github.com/pprzetacznik/pythonTestPackage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deepLearnR samples\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "library(deepLearnR)\n",
    "\n",
    "{\n",
    "library(MASS)\n",
    "data(Boston)\n",
    "X <- Boston[,2:14]\n",
    "y <- Boston[,1]\n",
    "TensorFlowDNNRegressor(modelTag=\"tfdnnr-01\", X=X, y=y, steps=5000)\n",
    "pred <- TensorFlow.regressorEval(modelTag=\"tfdnnr-01\")\n",
    "mse <- rPython::python.get(\"mse\")\n",
    "r2 <- rPython::python.get(\"r2\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "-1.077635512528"
      ],
      "text/latex": [
       "-1.077635512528"
      ],
      "text/markdown": [
       "-1.077635512528"
      ],
      "text/plain": [
       "[1] -1.077636"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$slope</dt>\n",
       "\t\t<dd>0.500460505485535</dd>\n",
       "\t<dt>$intercept</dt>\n",
       "\t\t<dd>0.269151866436005</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$slope] 0.500460505485535\n",
       "\\item[\\$intercept] 0.269151866436005\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$slope\n",
       ":   0.500460505485535\n",
       "$intercept\n",
       ":   0.269151866436005\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$slope\n",
       "[1] 0.5004605\n",
       "\n",
       "$intercept\n",
       "[1] 0.2691519\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "x.vals <- seq(1:100)\n",
    "y.vals <- 0.3 + 0.5 * x.vals\n",
    "lm.tf.fit <- deepLearnR::TensorFlow.SystemLinReg(X = x.vals, Y = y.vals,\n",
    "epochs = 100000, learning.rate = .00005)\n",
    "lm.tf.fit\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'tflr-03'"
      ],
      "text/latex": [
       "'tflr-03'"
      ],
      "text/markdown": [
       "'tflr-03'"
      ],
      "text/plain": [
       "[1] \"tflr-03\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.6312849\n",
      "[1] 0.6397306\n"
     ]
    }
   ],
   "source": [
    "Y <- deepLearnR::titanic.data$Survived\n",
    "X <- deepLearnR::titanic.data[,c(\"Age\",\"SibSp\",\"Fare\",\"Pclass\")]\n",
    "X$Age[is.na(X$Age)] <- mean(X$Age,na.rm=TRUE)\n",
    "set.seed(512)\n",
    "inTrain <- sample(1:nrow(X), trunc(nrow(X)*0.8))\n",
    "X.Train <- X[inTrain,]\n",
    "Y.Train <- Y[inTrain]\n",
    "X.Test <- X[-inTrain,]\n",
    "Y.Test <- Y[-inTrain]\n",
    "deepLearnR::TensorFlow.Classifier(modelTag=\"tflr-03\",X=X.Train,Y=Y.Train,steps=5000)\n",
    "pred <- deepLearnR::TensorFlow.predict(modelTag=\"tflr-03\",X=X.Test,Y=Y.Test)\n",
    "accuracy <- sum(pred == Y.Test)/length(Y.Test)\n",
    "print(accuracy) # Should be ~ 0.6312849\n",
    "pred <-  deepLearnR::TensorFlow.predict(modelTag=\"tflr-03\",X=X,Y=Y)\n",
    "accuracy <- sum(pred == Y)/length(Y)\n",
    "print(accuracy) # Should be ~ 0.6397306"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'tfdnnr-01'"
      ],
      "text/latex": [
       "'tfdnnr-01'"
      ],
      "text/markdown": [
       "'tfdnnr-01'"
      ],
      "text/plain": [
       "[1] \"tfdnnr-01\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "library(MASS)\n",
    "data(Boston)\n",
    "\n",
    "X <- Boston[,2:14]\n",
    "y <- Boston[,1]\n",
    " \n",
    "deepLearnR::TensorFlowDNNRegressor(modelTag=\"tfdnnr-01\", X=X, y=y, steps=5000)\n",
    "pred <- deepLearnR::TensorFlow.regressorEval(modelTag=\"tfdnnr-01\")\n",
    "mse <- rPython::python.get(\"mse\")\n",
    "r2 <- rPython::python.get(\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$slope</dt>\n",
       "\t\t<dd>0.500474572181702</dd>\n",
       "\t<dt>$intercept</dt>\n",
       "\t\t<dd>0.268206417560577</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$slope] 0.500474572181702\n",
       "\\item[\\$intercept] 0.268206417560577\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$slope\n",
       ":   0.500474572181702\n",
       "$intercept\n",
       ":   0.268206417560577\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$slope\n",
       "[1] 0.5004746\n",
       "\n",
       "$intercept\n",
       "[1] 0.2682064\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.vals <- seq(1:100)\n",
    "y.vals <- 0.3 + 0.5 * x.vals\n",
    "lm.tf.fit <- deepLearnR::TensorFlow.SystemLinReg(X = x.vals, Y = y.vals, \n",
    "                                     epochs = 100000, learning.rate = .00005)\n",
    "lm.tf.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run skflow/examples/mnist.py\n",
    "\n",
    "```\n",
    "(jupyter)[pankracy@piotrek-fedora examples]$ python mnist.py \n",
    "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
    "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
    "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
    "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
    "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
    "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
    "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
    "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
    "Step #1, avg. loss: 2.64234\n",
    "Step #101, avg. loss: 1.90145\n",
    "Step #201, avg. loss: 1.36286\n",
    "Step #301, avg. loss: 1.10363\n",
    "Step #401, avg. loss: 0.95203\n",
    "Step #501, avg. loss: 0.85592\n",
    "Step #601, epoch #1, avg. loss: 0.78724\n",
    "Step #701, epoch #1, avg. loss: 0.73279\n",
    "Step #801, epoch #1, avg. loss: 0.70121\n",
    "Step #901, epoch #1, avg. loss: 0.65916\n",
    "Accuracy: 0.860700\n",
    "Step #1, avg. loss: 3.32872\n",
    "Step #2001, epoch #3, avg. loss: 1.20425\n",
    "Step #4001, epoch #7, avg. loss: 0.43666\n",
    "Step #6001, epoch #10, avg. loss: 0.32565\n",
    "Step #8001, epoch #14, avg. loss: 0.27147\n",
    "Step #10001, epoch #18, avg. loss: 0.23350\n",
    "Step #12001, epoch #21, avg. loss: 0.20529\n",
    "Step #14001, epoch #25, avg. loss: 0.18313\n",
    "Step #16001, epoch #29, avg. loss: 0.16682\n",
    "Step #18001, epoch #32, avg. loss: 0.15348\n",
    "\n",
    "Accuracy: 0.968500\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R example\n",
    "==============\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/ml/datasets/Libras+Movement\n",
    "\n",
    "We try to boost our svm classifier with clustering (hclust). We make hierarchical clustering on our dataset. We cut on level of 40 clusters and try to label our clusters with specific label if every point (of train set) in such cluster have the same label. If this is true, we label our test point with such label. Rest of the points we classify with svm classifier.\n",
    "\n",
    "We compare this result with standard svm approach.\n",
    "\n",
    "\n",
    "We can visualize our dataset with help of PCA analysis.\n",
    "![Plot 2](img/Rplot2.png)\n",
    "![Plot 3](img/Rplot3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "library(lattice)\n",
    "library(class)\n",
    "library(kernlab)\n",
    "\n",
    "drawData <- function(Raw, RawData, Classes) {\n",
    "  temp <- lda(V91 ~ . , data=Raw)\n",
    "  ldax <- predict(temp, RawData)$x\n",
    "  xyplot(ldax[,1] ~ ldax[,2] + ldax[,3] + ldax[,4], groups=Classes, pch=19)\n",
    "\n",
    "  dataMatrix <- as.matrix(RawData)\n",
    "  rownames(dataMatrix) <- Classes\n",
    "  pca <- princomp(dataMatrix )\n",
    "  biplot(pca)\n",
    "\n",
    "  pc.cr <- prcomp(Raw)\n",
    "  pca.plot <- xyplot(pc.cr$x[,7] ~ pc.cr$x[,1], groups=Classes)\n",
    "  pca.plot$xlab <- \"First Component\"\n",
    "  pca.plot$ylab <- \"Second Component\"\n",
    "  pca.plot\n",
    "}\n",
    "\n",
    "getNewTrain <- function(train, classes, clustersNum, verbose) {\n",
    "  d <- dist(Raw, method = \"euclidean\")\n",
    "  fit <- hclust(d, method=\"ward\") \n",
    "  groups <- cutree(fit, k=clustersNum)\n",
    "  if (verbose == TRUE) {\n",
    "    plot(fit)\n",
    "    rect.hclust(fit, k=clustersNum, border=\"red\")\n",
    "  }\n",
    "  \n",
    "  for ( i in 1:clustersNum ) {\n",
    "    cluster <- as.integer(names(groups[groups == i]))\n",
    "    clusterTrainSubset <- cluster[cluster %in% train]\n",
    "    if ( length( clusterTrainSubset ) != 0 ) {\n",
    "      lider <- classes[clusterTrainSubset][1]\n",
    "      if ( all( classes[clusterTrainSubset] == lider ) ) {\n",
    "        newItems <- cluster[!cluster %in% clusterTrainSubset]\n",
    "        if ( length(newItems) != 0) {\n",
    "          train <- c(train, newItems)\n",
    "          classes[ newItems ] <- lider\n",
    "          if ( verbose == TRUE) {\n",
    "            cat(\"added items\", newItems, \"cluster:\", cluster, \"clusterTrainSubset:\", clusterTrainSubset, \"to class\", lider,\"\\n\")\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  return (list(\"train\" = train, \"classes\" = classes))\n",
    "}\n",
    "\n",
    "getResults <- function(Raw, RawData, Classes, train, clustersNum, maxPcaDim, verbose) {\n",
    "  filter <- ksvm(V91~.,data=Raw[train,],kernel=\"laplacedot\",kpar=list(sigma=0.05),C=5,cross=3)\n",
    "  result1 <- predict(filter, Raw[-train,-91])\n",
    "  table1 <- table(round(result1), Raw[-train,91])\n",
    "\n",
    "  pc <- prcomp(RawData)\n",
    "  pc$x\n",
    "  transformed <- as.matrix(pc$x[,1:maxPcaDim])\n",
    "  transformed <- cbind(Classes, transformed)\n",
    "  \n",
    "  newTrain <- getNewTrain(train, Classes, clustersNum, verbose)\n",
    "  trainDiff <- newTrain$train[! newTrain$train %in% train]\n",
    "  \n",
    "  filter <- ksvm(Classes~.,data=transformed[newTrain$train,],kernel=\"laplacedot\",kpar=list(sigma=0.05),C=5,cross=3)\n",
    "  result2 <- predict(filter, as.matrix(transformed[-newTrain$train,-1]))\n",
    "  table2 <- table( c(round(result2), newTrain$classes[trainDiff]), c(transformed[-newTrain$train,1], Classes[trainDiff]) )\n",
    "\n",
    "  return (c(sum(diag(table1))/sum(table1), sum(diag(table2))/sum(table2)))\n",
    "}\n",
    "\n",
    "drawResults <- function(Raw, RawData, Classes, classesDim, pcaDim, clustersNum, trainRatio, verbose) {\n",
    "  result <- matrix(0, ncol=2, nrow=5)\n",
    "  for ( j in 1:5 ) {\n",
    "    for ( k in 1:5 ) {\n",
    "      train <- vector()\n",
    "      for ( i in 1:classesDim ) {\n",
    "        buffer <- rownames(Raw[Raw[,91] == i,])\n",
    "        train <- as.integer(c(train, sample(buffer, round(length(buffer) * (trainRatio + k/10)))))\n",
    "      }\n",
    "      result[k,] <- result[k,] + getResults(Raw, RawData, Classes, train, clustersNum, pcaDim, verbose)\n",
    "    }\n",
    "  }\n",
    "  print(result <- result / 5)\n",
    "\n",
    "  plot(1:5, result[,1], type=\"o\", col=\"blue\", ylim=c(0,1))\n",
    "  points(1:5,result[,2], type=\"o\", col=\"red\")\n",
    "}\n",
    "\n",
    "Raw <- read.csv(\"libras_all.data\",header=FALSE)\n",
    "\n",
    "classesDim <- 10\n",
    "pcaDim <- 10\n",
    "clustersNum <- 40\n",
    "trainRatio <- 10/100\n",
    "\n",
    "Raw <- Raw[Raw[,91] %in% 1:classesDim,]\n",
    "RawData <- Raw[,1:90]\n",
    "Classes <- Raw[,91]\n",
    "\n",
    "drawResults(Raw, RawData, Classes, classesDim, pcaDim, clustersNum, trainRatio, FALSE)\n",
    "\n",
    "drawData(Raw, RawData, Classes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "===============\n",
    "\n",
    "Red line is svm supported by clustering. Blue line is standard svm approach. We can see that svm supported with unsupervised learning \n",
    "\n",
    "![Plot 1](img/Rplot1.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
